---
title: 'Lesson 1 - Exploratory Data Analysis: Import, Inspect, and Visualize Data'
author: "Zach Budesa"
date: "2023-05-16"
output: html_document
---

# Set up, load packages, import data

The first thing you will always need to do is make sure that you have the correct packages installed, and ready to use. This first block is code that will install them if you don't have them. You can see that the code in this block isn't set to run! By adding `#` anywhere in a line of code, nothing following `#` will run. This is a useful tool we'll use throughout our work. Commenting is a key tool for making code that other people can use. We'll talk more about it later.

```{r}
# Use if you have never ran any R code using {readr} or {tidyverse}
# packages <- c("readr", "tidyverse")
# install.packages(packages)
```

Once you are sure that you have the correct packages installed, this block will load both of the packages you need. You'll see here that {tidyverse} is the last package loaded. Because the Tidyverse as a whole consists of multiple package, it is always useful to load. The thing to keep in mind is that the functions you will come to know and love from the Tidyverse are used in a lot of other packages! If you load {tidyverse}, then other packages, those packages may have functions that use the same call, but do something completely different. By loading it last, you can prevent code breaking problems!

We're also using {readr} to load our data. There are myraid options for loading data in R, and readr is one that's useful for .csvs. But! How do you know which data package you need can be a challenge.

```{r}
library(readr)
library(tidyverse)
```

Loading these packages gives you some good information, and tells you what the conflicts {tidyverse} has. It conflicts with the {stats} base package. The best way to avoid this is to load {tidyverse} *last*.

## Loading Data

Let's start with loading data into R. It's *easy*! Unless it's not. The single line of code below will load the data named `training-scales.csv` contained in the `data` folder. But sometimes it's not as obvious the best way to load data. Until you become familiar with the different packages and data types R can handle, you may need to cheat. In the bottom right corner or RStudio, you can navigate to the `data` folder by clicking the name. Then, click the `training-scales.csv` file, which will give you the option to "View File" or "Import Dataset..." Selecting "Import Dataset..." will give you a dialogue box that shows you what your file looks like, gives you options for importing the data, and gives you the code you need. You can use this dialogue box, but I recommend using the "Code Preview" and copying the code to paste into your own script. With other types of data files, this preview won't work, so only use it as an aide.

```{r}
training_data <- read_csv("data/training-scales.csv")
```

# Inspect your data

Now, let's look at our data.

```{r}
training_data
```

The RMarkdown file that we are using gives you a built in data viewer. You can explore each column and row, but you're fairly limited. You get 10 rows and 10 columns per page and that's just not enough! Instead, RStudio's built in viewer is a bit more flexible:

```{r}
View(training_data)
```

There are a few more ways that we can inspect our data. Try out each of the following functions and see how they represent the data in a new way.

```{r}
# Glimpse lets you do just that. You can get a broad overview of the size of the data, column names, data types, and the first few rows.
glimpse(training_data)

# Dim gives you the dimensions--numbers of rows and columns--in the data. This is especially useful when you need to check that a transformation didn't change your dataset in a way you didn't intend.
dim(training_data)
training_data <- training_data %>% mutate(leo = as_factor(leo))
# Summary is a common function you will use a lot. For a dataset/data frame, if gives you basic information about each column, based on it's type.
# Numeric - For columns of numbers, it gives you minimum, maximum, mean, and 25th and 7th quartiles.
# Data - Same as for numeric columns, but minimum corresponds to earlier (longer ago), and maximum is more recent.
# Character - Only shows that it is a character type.
# Category - For categorical/ordinal variables, each level/category is displayed and the count in that level is provided.
summary(training_data)

# Finaly, the head and tail functions give you the first/last 10 rows. You can even change the number of rows it gives you by providing an `n = ` with the number of rows you want.
head(training_data) ;tail(training_data, n = 3)
```
## Plotting data
You should never only look at number! Plotting data is a great method to learn more about your data. We'll practice more with this later, but the following functions are always useful for getting a sense of what your data look like. We'll spend more time with {ggplot2}, the corvette of R plotting shortly. But for now, let's start with some base R functions: `plot` and `hist`.

Plot is the all purpose workhorse built into R. Most packages have an addition to `plot` that makes it useful for whatever type of data you have. Below are a few examples of how you can use this. First, you can plot all values of a single variable. Next, `plot` makes bivariate scatter plots when supplied with two numeric variables. Finally, plot can provide plots of analyses under most situations.
```{r}
# Single variable Plot
# The $ is called an atomic operator. You can select a smaller piece of a larger object with it. In this case, the data `training_data` is the large object, and column `year_field` is the smaller object.
plot(training_data$year_field)

# Bivariate Scatter Plot
# Here `with` tells R which object to use, then you only supply plot with the columns of interest. 
#`rank` transforms the `duration_in_seconds` column into a rank ordered variable.
with(training_data, 
     plot(rank(duration_in_seconds), age)
     )
# `lm` performs and OLS linear regression, and `plot` produces summary plots.
plot(
  lm(training_data$burnout1 ~ training_data$burnout2)
  )
```

Histograms are another useful way to evaluate data. A histogram "bins" data and gives you a count of the number of responses per bin. You can see a few examples below. 
```{r}
# This is an unattractive histogram of one of the variables from our dataset. 
hist(training_data$ooas6)

# This will generate a histogram of a random normal distribution.
hist(
  # `rnorm` produces n random draws that requires two moments: mean and standard deviation. 
  rnorm(
    # The number of draws
    n = 500,
    # The mean, or average, which is where a normal distribution is centred.
    mean = 2.5, 
    # The standard deviation, or spread of a distribution. We'll talk about it more shortly.
    sd = .7)
)

```

# Investigating Scores
Now that we've discussed some ways to inspect data, let's talk about summarizing it. As noted above, mean and standard deviation are useful ways to summarize many distributions. They also have very easy to remember functions in R: `mean` gives you an arithmetic mean, and `sd` computes standard deviations. To start, let's look at how you can obtain them. This first method is a way to obtain as many summary statistics of numeric data as possible, but it doesn't work. What is the error?
```{r}
training_data %>% 
  summarise(duration_mean = mean(duration_in_seconds),
            duration_sd = sd(duration_in_seconds),
            burnout1_mean = mean(burnout1),
            burnout1_sd = sd(burnout1))

```
Many of our variable has missing data. R's default is to not anticipate msising data. This is a good thing! It can be annoying, but not knowing you have missing data is a problem. That being said, you still need a way to obtain summary statistics despite this! Here's how you tell functions like `mean` and `sd` to ignore them:
```{r}
training_data %>% 
  summarise(duration_mean = mean(duration_in_seconds),
            duration_sd = sd(duration_in_seconds),
            burnout1_mean = mean(burnout1, na.rm = TRUE),
            burnout1_sd = sd(burnout1, na.rm = TRUE))
```
We can also use the `group_by` Tidyverse function to get these estimates by group:
```{r}
training_data %>% 
  group_by(leo) %>% 
  summarise(duration_mean = mean(duration_in_seconds),
            duration_sd = sd(duration_in_seconds),
            burnout1_mean = mean(burnout1, na.rm = TRUE),
            burnout1_sd = sd(burnout1, na.rm = TRUE))
```

### A brief stats lesson: Standard Deviation & IQR, Simple Measures of Spread

#### Standard Deviation
Standard deviation is a measure of data dispersal. The higher it is, the further spread apart data points are. It's also the square root of variance. See the image and equation below.
$$
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{\Sigma(x_i - \mu)^2}{N}}
$$ 

![sd image](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/1200px-Standard_deviation_diagram.svg.png)

#### Interquartile Range
The interquartile range is another measure of dispersion. It is much more applicable to medians rather than means, but it can also be useful. Here is how it is calculated:
```{r}
x <- seq(1:10)
x

median(x)

quantile(x)

7.75-3.25
IQR(x)

```
\end stats break

## An alternative summarization
The above method for summarizing works well, but it can be tedious if you want a lot of individual summary statistics--you have to code each one separately! Instead, learning how to pivot data is a great way to get a lot more summary stats quickly:
```{r}
training_data %>% 
  pivot_longer(burnout1:burnout9) %>% 
  
  group_by(name) %>% 
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            med = median(value, na.rm = TRUE),
            IQR = IQR(value, na.rm = TRUE))
```



## Missing Data
Missing data is a problem. We won't spend time now on it, but here are a few functions we can use to investigate the amount of missing data. And, we'll build our first function! Don't worry yet about how functions work, you can copy and paste this one as much as you need!
```{r}
sum(complete.cases(training_data))

sum(is.na(training_data))

sum(!is.na(training_data))

missing_pct <- function(data){
  # The total count of missing data
  sum(is.na(data))/
    # count of all data (missing + non-missing)
    (sum(is.na(data)+!is.na(data)))
}

missing_pct(training_data)
```

# Visualize
Visualization may be more important for investigating data than using numbers. Sometimes numbers can be misleading, and if you don't graph your data, you'll never know. In this section, we'll talk about ways that you can start graphing your data that's both nice looking and useful.

## Scatter plots
Scatter plots. We've already looked at what a scatter plot is above, but now let's use {ggplot2} to make even fancier ones.

As we've mentioned, a scatter plot is a way to graph bivariate relationships. It plots a single data point at each combination of two variables on the x- and y-axis. First, let's start with a simple scatter plot. The code below may use some notation and symbols you are not familiar with, but don't worry about that for now.

For these graphs, we're using {ggplot2}. It's part of the Tidyverse, so you don't need to load any packages. What you do need to know is that ggplot uses a very specific notation that we'll have to practice.
```{r}
# Data always comes first. The symbol here is the pipe. We'll talk about it later in more depth.
training_data %>% 
  # For every plot, you have to have a canvas. the `ggplot` function is your canvas.
  ggplot() +
  # Now we tell R that we what points on top of our canvas.
  geom_point(
    # aes stands for "Aesthetic"; what do we want our points made of?
    aes(
      # Here we assign variables to each axis.
      x = age, 
      y = year_field))
```

This is a basic scatter plot. It shows a clear positive relationships. This makes sense, you can't be in a field longer than you've been alive! This is a useful way to check our data: do any of our participants report having been first responders longer than they've been alive?

### Basic Customizations
Now, our participants are not all the same. Some of them are law enforcement officers (LEO), some are emergency medical service personnel (EMS). In our data, this is represented by the leo variable, where 1 = LEO and 0 = EMS. We can use the color argument to help use see which is which.

```{r}
training_data %>% 
  ggplot() +
  # Here we've added the color argument inside the `aes` function.
  geom_point(aes(x = age, y = year_field, color = factor(leo)))
```

Maybe it would be easier to look at our participants' age:tenure relationship more separately. I'm slightly concerned that we can't see everyone very well, are there really that few EMS? We can separate out plots using the `facet_wrap` function. 
```{r}
training_data %>% 
  ggplot() +
  geom_point(aes(x = age, y = year_field, color = factor(leo))) +
  # `facet_wrap` is useful to separate single plots into multiple facets.
  facet_wrap(~ leo)
```

There, now we can see that this relationship is fairly similar across both types of participants. What other variables do we have that we can use to facet our scatter plots with?

## Bar Charts
Bar charts or bar graphs are another great way to visualize data. A bar chart is good to get a sense of the number of counts within a category. Below, you can see how the bar chart shows us how many participants selected each gender category.
```{r}
training_data %>% 
  ggplot() +
  # Because bar charts display counts, only a single variable is required.
  geom_bar(aes(x = gend))
```

Part of what makes {ggplot2} such a useful tools is that it is infinitely customizable. We can apply the same use of `facet_wrap` to a bar chart just as easily as to a scatter plot
```{r}
training_data %>% 
  ggplot() +
  geom_bar(aes(x = gend)) +
  # Now we add the same line of code as above.
  facet_wrap(~ leo)
```

>Brief data viz break: Turning a bar chart on it's side is a great way to make bar charts nicer to look at and easier to read. 

```{r}
training_data %>% 
  ggplot() +
  geom_bar(aes(x = gend, fill = gend)) +
  facet_wrap(~ leo) +
  coord_flip()
```

## Box and Violin Plots

The last type of visualizations we'll cover are box and violin plots. These are roughly the same in their use, but are good for different things. A box plot will show you median, IQR, and range, as below. 
```{r}
training_data %>% 
  ggplot() +
  # Here I've gone ahead and added color to make it more interesting.
  geom_boxplot(aes(x = ethnicity, y = age, fill = ethnicity))
```

A violin plot does the same, except it also represents the density of data points. Thicker violin plots means more data points, thinner means fewer data points. Does the violin plot below provide you with any additional information about the relationship between participants' age and ethnicity?
```{r}
training_data %>% 
  ggplot() +
  geom_violin(aes(x = ethnicity, y = age, fill = ethnicity))
```

These types of plots represent data in related, but different ways. You may even see them done together. Because {ggplot2} is like painting, you can combine multiple visualizations in a single plot. Notice how the code below supplies the needed variable information to both plot types.
```{r}
training_data %>% 
  ggplot() +
  geom_violin(aes(x = ethnicity, y = age, fill = ethnicity)) +
  geom_boxplot(aes(x = ethnicity, y = age), width = .3)
```

A nice part about {ggplot2} is that you can supply your graphs with information in multiple ways. This is the same graph, but notice that I placed most of my variable information in the initial "canvas" function.
```{r}
training_data %>% 
  ggplot(aes(x = ethnicity, y = age)) +
  geom_violin(aes(fill = ethnicity)) +
  geom_boxplot(width = .3)
```

## Getting Crazy with {ggplot2}
There is no end of customizability with Tidyverse functions and {ggplot2}. Below is a small example of adding multiple elements to a single chart. Is it great Data Viz? Probably not.

```{r}
# Wrangle data
training_data %>% 
  filter(!is.na(ethnicity)) %>% 
  mutate(long = case_when(
    duration_in_seconds <= 900 ~ "Less than 15 Minutes",
    duration_in_seconds > 900 ~ "Greater than 15 Minutes"
  )) %>% 
  
  # Plot data
  ggplot(aes(x = ethnicity, y = age)) +
  geom_violin(aes(fill = ethnicity)) +
  geom_jitter(alpha = .3) +
  geom_boxplot(width = .3) +
  cowplot::theme_cowplot() + 
  ggtitle("Distribution of Age by Ethnicity",
          subtitle = "Faceted by how long it took for participants to complete the survey.") +
  labs(x = "Participants' Reported Ethnicity",
       y = "Participants' Age in Years",
       caption = "This graph uses jitter to spread age data out for interpretability of data points.") +
  theme(legend.position = "none") + 
  facet_wrap(~ long,
             strip.position = 'bottom',
             scales = "free"
             )
  
  
```

##### Session Info

```{r}
sessionInfo()
```
