---
title: 'Lesson 1 - Exploratory Data Analysis: Import, Inspect, and Visualize Data'
author: "Zach Budesa"
date: "2023-05-16"
output: html_document
---

# Set up, load packages, import data

The first thing you will always need to do is make sure that you have the correct packages installed, and ready to use. This first block is code that will install them if you don't have them. You can see that the code in this block isn't set to run! By adding `#` anywhere in a line of code, nothing following `#` will run. This is a useful tool we'll use throughout our work. Commenting is a key tool for making code that other people can use. We'll talk more about it later.

```{r}
# Use if you have never ran any R code using {readr} or {tidyverse}
# packages <- c("readr", "tidyverse")
# install.packages(packages)
```

Once you are sure that you have the correct packages installed, this block will load both of the packages you need. You'll see here that {tidyverse} is the last package loaded. Because the Tidyverse as a whole consists of multiple package, it is always useful to load. The thing to keep in mind is that the functions you will come to know and love from the Tidyverse are used in a lot of other packages! If you load {tidyverse}, then other packages, those packages may have functions that use the same call, but do something completely different. By loading it last, you can prevent code breaking problems!

We're also using {readr} to load our data. There are myraid options for loading data in R, and readr is one that's useful for .csvs. But! How do you know which data package you need can be a challenge.

```{r}
library(readr)
library(tidyverse)
```

Loading these packages gives you some good information, and tells you what the conflicts {tidyverse} has. It conflicts with the {stats} base package. The best way to avoid this is to load {tidyverse} *last*.

## Loading Data

Let's start with loading data into R. It's *easy*! Unless it's not. The single line of code below will load the data named `training-scales.csv` contained in the `data` folder. But sometimes it's not as obvious the best way to load data. Until you become familiar with the different packages and data types R can handle, you may need to cheat. In the bottom right corner or RStudio, you can navigate to the `data` folder by clicking the name. Then, click the `training-scales.csv` file, which will give you the option to "View File" or "Import Dataset..." Selecting "Import Dataset..." will give you a dialogue box that shows you what your file looks like, gives you options for importing the data, and gives you the code you need. You can use this dialogue box, but I recommend using the "Code Preview" and copying the code to paste into your own script. With other types of data files, this preview won't work, so only use it as an aide.

```{r}
training_data <- read_csv("data/training-scales.csv")
```

# Inspect your data

Now, let's look at our data.

```{r}
training_data
```

The RMarkdown file that we are using gives you a built in data viewer. You can explore each column and row, but you're fairly limited. You get 10 rows and 10 columns per page and that's just not enough! Instead, RStudio's built in viewer is a bit more flexible:

```{r}
View(training_data)
```

There are a few more ways that we can inspect our data. Try out each of the following functions and see how they represent the data in a new way.

```{r}
# Glimpse lets you do just that. You can get a broad overview of the size of the data, column names, data types, and the first few rows.
glimpse(training_data)

# Dim gives you the dimensions--numbers of rows and columns--in the data. This is especially useful when you need to check that a transformation didn't change your dataset in a way you didn't intend.
dim(training_data)
training_data <- training_data %>% mutate(leo = as_factor(leo))
# Summary is a common function you will use a lot. For a dataset/data frame, if gives you basic information about each column, based on it's type.
# Numeric - For columns of numbers, it gives you minimum, maximum, mean, and 25th and 7th quartiles.
# Data - Same as for numeric columns, but minimum corresponds to earlier (longer ago), and maximum is more recent.
# Character - Only shows that it is a character type.
# Category - For categorical/ordinal variables, each level/category is displayed and the count in that level is provided.
summary(training_data)

# Finaly, the head and tail functions give you the first/last 10 rows. You can even change the number of rows it gives you by providing an `n = ` with the number of rows you want.
head(training_data) ;tail(training_data, n = 3)
```
## Plotting data
You should never only look at number! Plotting data is a great method to learn more about your data. We'll practice more with this later, but the following functions are always useful for getting a sense of what your data look like. We'll spend more time with {ggplot2}, the corvette of R plotting shortly. But for now, let's start with some base R functions: `plot` and `hist`.

Plot is the all purpose workhorse built into R. Most packages have an addition to `plot` that makes it useful for whatever type of data you have. Below are a few examples of how you can use this. First, you can plot all values of a single variable. Next, `plot` makes bivariate scatter plots when supplied with two numeric variables. Finally, plot can provide plots of analyses under most situations.
```{r}
# Single variable Plot
# The $ is called an atomic operator. You can select a smaller piece of a larger object with it. In this case, the data `training_data` is the large object, and column `year_field` is the smaller object.
plot(training_data$year_field)

# Bivariate Scatter Plot
# Here `with` tells R which object to use, then you only supply plot with the columns of interest. 
#`rank` transforms the `duration_in_seconds` column into a rank ordered variable.
with(training_data, 
     plot(rank(duration_in_seconds), age)
     )
# `lm` performs and OLS linear regression, and `plot` produces summary plots.
plot(
  lm(training_data$burnout1 ~ training_data$burnout2)
  )
```

Histograms are another useful way to evaluate data. A histogram "bins" data and gives you a count of the number of responses per bin. You can see a few examples below. 
```{r}
# This is an unattractive histogram of one of the variables from our dataset. 
hist(training_data$ooas6)

# This will generate a histogram of a random normal distribution.
hist(
  # `rnorm` produces n random draws that requires two moments: mean and standard deviation. 
  rnorm(
    # The number of draws
    n = 500,
    # The mean, or average, which is where a normal distribution is centred.
    mean = 2.5, 
    # The standard deviation, or spread of a distribution. We'll talk about it more shortly.
    sd = .7)
)

```

# Investigating Scores
Now that we've discussed some ways to inspect data, let's talk about summarizing it. As noted above, mean and standard deviation are useful ways to summarize many distributions. They also have very easy to remember functions in R: `mean` gives you an arithmetic mean, and `sd` computes standard deviations. To start, let's look at how you can obtain them. This first method is a way to obtain as many summary statistics of numeric data as possible, but it doesn't work. What is the error?
```{r}
training_data %>% 
  summarise(duration_mean = mean(duration_in_seconds),
            duration_sd = sd(duration_in_seconds),
            burnout1_mean = mean(burnout1),
            burnout1_sd = sd(burnout1))

```
Many of our variable has missing data. R's default is to not anticipate msising data. This is a good thing! It can be annoying, but not knowing you have missing data is a problem. That being said, you still need a way to obtain summary statistics despite this! Here's how you tell functions like `mean` and `sd` to ignore them:
```{r}
training_data %>% 
  summarise(duration_mean = mean(duration_in_seconds),
            duration_sd = sd(duration_in_seconds),
            burnout1_mean = mean(burnout1, na.rm = TRUE),
            burnout1_sd = sd(burnout1, na.rm = TRUE))
```
We can also use the `group_by` Tidyverse function to get these estimates by group:
```{r}
training_data %>% 
  group_by(leo) %>% 
  summarise(duration_mean = mean(duration_in_seconds),
            duration_sd = sd(duration_in_seconds),
            burnout1_mean = mean(burnout1, na.rm = TRUE),
            burnout1_sd = sd(burnout1, na.rm = TRUE))
```

### A brief stats lesson: Standard Deviation & IQR, Simple Measures of Spread

#### Standard Deviation
Standard deviation is a measure of data dispersal. The higher it is, the further spread apart data points are. It's also the square root of variance. See the image and equation below.
$$
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{\Sigma(x_i - \mu)^2}{N}}
$$ 

![sd image](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/1200px-Standard_deviation_diagram.svg.png)

#### Interquartile Range
The interquartile range is another measure of dispersion. It is much more applicable to medians rather than means, but it can also be useful. Here is how it is calculated:
```{r}
x <- seq(1:10)
x

median(x)

quantile(x)

7.75-3.25
IQR(x)

```
\end stats break

## An alternative summarization
The above method for summarizing works well, but it can be tedious if you want a lot of individual summary statistics--you have to code each one separately! Instead, learning how to pivot data is a great way to get a lot more summary stats quickly:
```{r}
training_data %>% 
  pivot_longer(burnout1:burnout9) %>% 
  
  group_by(name) %>% 
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            med = median(value, na.rm = TRUE),
            IQR = IQR(value, na.rm = TRUE))
```

## Sum Scores
Finally, adding up scales scores is always useful. We won't always want item level scores, so sum scores are a great way to summarize every item together. The `rowSums` function does so. 
```{r}
# Using rowSums with `cbind`.
with(training_data,
     rowSums(cbind(burnout1, burnout2, burnout3,
                   burnout4, burnout5, burnout6,
                   burnout7, burnout8, burnout9), na.rm = TRUE)
)

# Adding them together
with(training_data,
     burnout1+burnout2+burnout3+burnout4+burnout5+burnout6+burnout7+burnout8+burnout9
)

# `cbind` produces a dataframe/array
with(training_data,
     cbind(burnout1, burnout2, burnout3,
                   burnout4, burnout5, burnout6,
                   burnout7, burnout8, burnout9)
     )

# Creating a burnout object
burnout <- training_data %>% 
  select(burnout1:burnout9)

# Adding sum scores to our burnout object
burnout <- training_data %>% 
  mutate(sum = rowSums(cbind(burnout1, burnout2, burnout3,
                              burnout4, burnout5, burnout6,
                              burnout7, burnout8, burnout9), na.rm = TRUE))

# Plotting our burnout sum scores.
plot(burnout$sum)
hist(burnout$sum)

```

## Missing Data
Missing data is a problem. We won't spend time now on it, but here are a few functions we can use to investigate the amount of missing data. And, we'll build our first function! Don't worry yet about how functions work, you can copy and paste this one as much as you need!
```{r}
sum(complete.cases(training_data))

sum(is.na(training_data))

sum(!is.na(training_data))

missing_pct <- function(data){
  # The total count of missing data
  sum(is.na(data))/
    # count of all data (missing + non-missing)
    (sum(is.na(data)+!is.na(data)))
}

missing_pct(training_data)
```

# Visualize

## Scatterplots

```{r}
training_data %>% 
  ggplot() +
  geom_point(aes(x = age, y = year_field))
```

```{r}
training_data %>% 
  ggplot() +
  geom_point(aes(x = age, y = year_field, color = factor(leo)))
```

```{r}
training_data %>% 
  ggplot() +
  geom_point(aes(x = age, y = year_field, color = factor(leo))) +
  facet_wrap(~ leo)
```

## Barcharts

```{r}
training_data %>% 
  ggplot() +
  geom_bar(aes(x = gend))
```

```{r}
training_data %>% 
  ggplot() +
  geom_bar(aes(x = gend)) +
  facet_wrap(~ leo)
```

```{r}
training_data %>% 
  ggplot() +
  geom_bar(aes(x = gend, fill = gend)) +
  facet_wrap(~ leo) +
  coord_flip()
```

## Box and Violin Plots

```{r}
training_data %>% 
  ggplot() +
  geom_boxplot(aes(x = ethnicity, y = age, fill = ethnicity))
```

```{r}
training_data %>% 
  ggplot() +
  geom_violin(aes(x = ethnicity, y = age, fill = ethnicity))
```

```{r}
training_data %>% 
  ggplot() +
  geom_violin(aes(x = ethnicity, y = age, fill = ethnicity)) +
  geom_boxplot(aes(x = ethnicity, y = age), width = .3)
```

```{r}
training_data %>% 
  ggplot(aes(x = ethnicity, y = age)) +
  geom_violin(aes(fill = ethnicity)) +
  geom_boxplot( width = .3)
```

##### Session Info

```{r}
sessionInfo()
```
